import openai
import os
from flask import Flask, request, jsonify, render_template
from sentence_transformers import SentenceTransformer
import faiss
import pickle
from tree_sitter import Language, Parser
import re
import logging

logging.basicConfig(level=logging.INFO)

# Configuration
BASE_DIR = os.getcwd()
PYTHON_FILES_DIR = os.path.join(BASE_DIR, "source_code")
FAISS_DB_DIR = os.path.join(BASE_DIR, "faiss_coarser_improved_db")
FAISS_CST_INDEX_FILE = os.path.join(FAISS_DB_DIR, "faiss_cst_functions.idx")
FAISS_TEXT_INDEX_FILE = os.path.join(FAISS_DB_DIR, "faiss_text_functions.idx")
CODE_SNIPPETS_FILE = os.path.join(FAISS_DB_DIR, "code_function_snippets.pkl")

# Ensure FAISS directory exists
os.makedirs(FAISS_DB_DIR, exist_ok=True)

# Locate README dynamically
README_FILE = None
for root, dirs, files in os.walk(PYTHON_FILES_DIR):
    if "README.md" in files:
        README_FILE = os.path.join(root, "README.md")
        break

# Load OpenAI API key from environment
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("Missing OpenAI API Key. Set OPENAI_API_KEY environment variable.")

# Initialize Flask App
app = Flask(__name__)

# Load Sentence Transformer model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Load Tree-sitter parser
PYTHON_LANGUAGE = Language("build/my-languages.so", "python")
parser = Parser()
parser.set_language(PYTHON_LANGUAGE)

# FAISS Setup
dimension = 384
faiss_cst_index = faiss.IndexFlatL2(dimension)
faiss_text_index = faiss.IndexFlatL2(dimension)
code_snippets = []
readme_text = ""

# Extract function and class definitions from a Python file
def extract_functions_and_classes(filepath, source_code):
    """Extracts function and class definitions from Python code."""
    tree = parser.parse(source_code.encode("utf-8"))
    root_node = tree.root_node

    # Get the source code lines for line number references
    source_lines = source_code.splitlines()

    functions_and_classes = []

    def get_node_text(node):
        start_point, end_point = node.start_point, node.end_point
        if start_point[0] == end_point[0]:
            # Single line
            return source_lines[start_point[0]][start_point[1]:end_point[1]]
        else:
            # Multiple lines
            result = [source_lines[start_point[0]][start_point[1]:]]
            for i in range(start_point[0] + 1, end_point[0]):
                result.append(source_lines[i])
            result.append(source_lines[end_point[0]][:end_point[1]])
            return '\n'.join(result)

    def get_definition_type(node):
        if node.type == "function_definition":
            # Find the function name node
            for child in node.children:
                if child.type == "identifier":
                    return "function", child.text.decode("utf-8")
        elif node.type == "class_definition":
            # Find the class name node
            for child in node.children:
                if child.type == "identifier":
                    return "class", child.text.decode("utf-8")
        return None, None

    def traverse_for_definitions(node):
        if node.type in ["function_definition", "class_definition"]:
            def_type, name = get_definition_type(node)
            if def_type and name:
                code_text = get_node_text(node)
                start_line = node.start_point[0] + 1  # 1-indexed line numbers
                end_line = node.end_point[0] + 1

                functions_and_classes.append({
                    "type": def_type,
                    "name": name,
                    "code": code_text,
                    "start_line": start_line,
                    "end_line": end_line,
                    "filepath": filepath
                })
                logging.info(f"Extracted {def_type}: {name} from {filepath} (lines {start_line}-{end_line})")
                # If it's a class, also extract its methods
                if def_type == "class":
                    for child in node.children:
                        if child.type == "block":
                            for method_node in child.children:
                                if method_node.type == "function_definition":
                                    method_type, method_name = get_definition_type(method_node)
                                    if method_type and method_name:
                                        method_code = get_node_text(method_node)
                                        method_start = method_node.start_point[0] + 1
                                        method_end = method_node.end_point[0] + 1

                                        functions_and_classes.append({
                                            "type": "method",
                                            "name": method_name,
                                            "class_name": name,
                                            "code": method_code,
                                            "start_line": method_start,
                                            "end_line": method_end,
                                            "filepath": filepath
                                        })
                                        logging.info(f"Extracted {def_type}: {name} from {filepath} (lines {start_line}-{end_line})")
        for child in node.children:
            traverse_for_definitions(child)

    traverse_for_definitions(root_node)
    return functions_and_classes

# Extract CST features
def extract_cst_features(source_code):
    """Extracts CST features from Python code."""
    tree = parser.parse(source_code.encode("utf-8"))
    root_node = tree.root_node
    cst_features = []

    def traverse(node):
        if node.child_count == 0:
            return
        cst_features.append(node.type)
        if node.type == "identifier":
            cst_features.append(node.text.decode("utf-8"))
        for child in node.children:
            traverse(child)

    traverse(root_node)
    return " ".join(cst_features)

# Get a descriptive title for the code snippet
def get_snippet_title(snippet):
    if snippet["type"] == "function":
        logging.info(f"Extracted function: {snippet['name']}")
        return f"Function: {snippet['name']}"
    elif snippet["type"] == "class":
        logging.info(f"Extracted class: {snippet['name']}")
        return f"Class: {snippet['name']}"
    elif snippet["type"] == "method":
        logging.info(f"Extracted method: {snippet['class_name']}.{snippet['name']}")
        return f"Method: {snippet['class_name']}.{snippet['name']}"
    return "Code Snippet"

# Extract comments and docstrings from the code snippet
def extract_comments_and_docstrings(code):
    comments = re.findall(r'#.*', code)
    docstring_match = re.search(r'\"\"\"(.*?)\"\"\"', code, re.DOTALL)
    docstring = docstring_match.group(1) if docstring_match else ""
    return comments, docstring

# Check and initialize FAISS indices if missing
if os.path.exists(FAISS_CST_INDEX_FILE) and os.path.exists(FAISS_TEXT_INDEX_FILE) and os.path.exists(CODE_SNIPPETS_FILE):
    print("âœ… Loading precomputed FAISS CST and text indexes...")
    faiss_cst_index = faiss.read_index(FAISS_CST_INDEX_FILE)
    faiss_text_index = faiss.read_index(FAISS_TEXT_INDEX_FILE)
    with open(CODE_SNIPPETS_FILE, "rb") as f:
        code_snippets = pickle.load(f)
    print(f"âœ… Loaded {len(code_snippets)} code snippets")
else:
    print("ğŸ”„ No FAISS indexes found. Initializing from scratch...")
    for root, _, files in os.walk(PYTHON_FILES_DIR):
        for filename in files:
            if filename.endswith(".py"):
                filepath = os.path.join(root, filename)
                try:
                    with open(filepath, "r", encoding="utf-8") as file:
                        source_code = file.read()
                        if not source_code.strip():
                            continue

                        # Extract functions and classes
                        extracted_items = extract_functions_and_classes(filepath, source_code)
                        print(f"ğŸ“„ Extracted {len(extracted_items)} functions/classes from {filepath}")

                        for item in extracted_items:
                            code = item["code"]

                            # Extract comments and docstrings
                            comments, docstring = extract_comments_and_docstrings(code)

                            # Create text embedding
                            text_embedding = embedding_model.encode(code, convert_to_numpy=True)
                            logging.info(f"Adding to FAISS text index: {item['type']} - {item['name']} from {item['filepath']}")
                            faiss_text_index.add(text_embedding.reshape(1, -1))

                            # Create CST embedding
                            cst_text = extract_cst_features(code)
                            cst_embedding = embedding_model.encode(cst_text, convert_to_numpy=True)
                            logging.info(f"Adding to FAISS CST index: {item['type']} - {item['name']} from {item['filepath']}")
                            faiss_cst_index.add(cst_embedding.reshape(1, -1))

                            # Add to code snippets
                            item["comments"] = comments
                            item["docstring"] = docstring
                            code_snippets.append(item)
                except Exception as e:
                    print(f"âŒ Error processing {filepath}: {str(e)}")

    print(f"âœ… Indexed {len(code_snippets)} functions and classes")

    faiss.write_index(faiss_text_index, FAISS_TEXT_INDEX_FILE)
    faiss.write_index(faiss_cst_index, FAISS_CST_INDEX_FILE)
    with open(CODE_SNIPPETS_FILE, "wb") as f:
        pickle.dump(code_snippets, f)

# Load README if available
if README_FILE and os.path.exists(README_FILE):
    with open(README_FILE, "r", encoding="utf-8") as f:
        readme_text = f.read()
        print("âœ… README.md loaded for project summary!")

def retrieve_code_snippets(query, top_k=10):
    """Retrieves the most relevant code snippets based on query."""
    if len(code_snippets) == 0:
        return None, "No code snippets available."

    query_embedding = embedding_model.encode(query, convert_to_numpy=True)
    distances_cst, indices_cst = faiss_cst_index.search(query_embedding.reshape(1, -1), top_k)
    distances_text, indices_text = faiss_text_index.search(query_embedding.reshape(1, -1), top_k)

    print(f"ğŸ” Query: {query}")
    print(f"ğŸ“ CST Distances: {distances_cst}")
    print(f"ğŸ“ Text Distances: {distances_text}")

    THRESHOLD = 2.0
    best_cst = distances_cst[0][0] if len(distances_cst[0]) > 0 else float('inf')
    best_text = distances_text[0][0] if len(distances_text[0]) > 0 else float('inf')

    retrieved_items = []

    if best_cst < THRESHOLD or best_text < THRESHOLD:  # Check if EITHER is below the threshold
        if best_text < best_cst:
            print("âš  Using text-based retrieval (distance: {:.4f})".format(best_text))
            #retrieved_indices = indices_text[0][:top_k]
            retrieved_indices = indices_cst[0][:top_k]
        else:
            print("âœ… Using CST-based retrieval (distance: {:.4f})".format(best_cst))
            retrieved_indices = indices_cst[0][:top_k]

        # Filter out invalid indices
        retrieved_indices = [idx for idx in retrieved_indices if idx < len(code_snippets)]

        # Get the actual snippets
        for idx in retrieved_indices:
            snippet = code_snippets[idx]
            retrieved_items.append(snippet)
            logging.info(f"Retrieved snippet: {snippet['type']} - {snippet['name']} from {snippet['filepath']}")

    if not retrieved_items:
        print("ğŸŒ No relevant local match found. Falling back to generic AI.")
        return None, "No relevant code found."

    # Format the retrieved snippets into a context string
    context_parts = []
    for snippet in retrieved_items:
        snippet_type = snippet["type"]
        name = snippet["name"]
        code = snippet["code"]
        filepath = snippet["filepath"]
        comments = "\n".join(snippet["comments"])
        docstring = snippet["docstring"]

        if snippet_type == "method":
            class_name = snippet["class_name"]
            header = f"Method: {class_name}.{name} from {filepath} (lines {snippet['start_line']}-{snippet['end_line']})"
        else:
            header = f"{snippet_type.capitalize()}: {name} from {filepath} (lines {snippet['start_line']}-{snippet['end_line']})"

        context_parts.append(f"{header}\n```python\n{code}\n```\n\nComments:\n{comments}\n\nDocstring:\n{docstring}\n")

    return None, "\n".join(context_parts)

def get_openai_response(question, context=None):
    try:
        # Updated system prompt
        system_prompt = (
            "You are an AI assistant for analyzing a Python project. "
            "Your goal is to provide detailed insights into the project's structure using the provided context. "
            "The context contains functions, classes, and methods from the codebase, along with associated comments and docstrings. "
            "For each item in the context:\n"
            "- Identify its type (class, function, method).\n"
            "- Provide its name and location (file path and line numbers).\n"
            "- Include a brief description based on available comments or docstrings, or state 'No description available' if none is provided.\n\n"
            "When answering, organize your response into sections for 'Classes' and 'Functions' and include any relevant methods under their respective classes. "
            "If the context does not provide the required information, respond with 'I cannot answer based on the given code snippets.' "
            "Avoid generating information not present in the context."
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Context:\n\n{context}\n\nQuestion: {question}\n\nAnswer:"}
        ]

        client = openai.OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.3  # Reduced temperature for more deterministic answers
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        import traceback
        print("âŒ OpenAI API call failed:", traceback.format_exc())
        return f"OpenAI API error: {str(e)}"


def filter_irrelevant_info(response, context):
    # Split the response into sentences
    sentences = response.split(". ")

    relevant_sentences = []
    for sentence in sentences:
        # Check if the sentence contains any information from the context
        if any(snippet in sentence for snippet in context.split("\n")):
            relevant_sentences.append(sentence)

    # Join the relevant sentences back into a coherent response
    filtered_response = ". ".join(relevant_sentences)

    return filtered_response


@app.route("/query", methods=["POST"])
def handle_query():
    try:
        data = request.json
        question = data.get("question", "")
        if not question:
            return jsonify({"error": "No question provided"}), 400
        print(f"ğŸ“ Received question: {question}")

        # Create a base context with the README information
        context = f"Project Overview from README:\n{readme_text}\n\n" if readme_text else ""

        # Get relevant code snippets
        _, code_context = retrieve_code_snippets(question)
        if code_context != "No relevant code found." and code_context != "No code snippets available.":
            context += f"Relevant Code Snippets:\n\n{code_context}"
            print(f"ğŸ“„ Found relevant code snippets")
        elif context:
            print("ğŸ“„ Using README context only")
        else:
            print("âš ï¸ No context available")
        response = get_openai_response(question, context)
        filtered_response = filter_irrelevant_info(response, context)

        return jsonify({"question": question, "answer": filtered_response})


    except Exception as e:
        print(f"âŒ Server Error: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route("/")
def index():
    return render_template("index.html")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)